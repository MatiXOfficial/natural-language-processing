{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82f83eed-555f-4288-a78e-ffa082b7c621",
   "metadata": {},
   "source": [
    "# Przetwarzanie języka naturalnego – lab5\n",
    "## Mateusz Kocot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5636afe-a8a1-442a-ae00-99cb6509f9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b47ac6-3ff5-41fe-a494-90d4a55290ec",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10d0ca8-be56-4cfa-9168-d4ac24d9818b",
   "metadata": {},
   "source": [
    "xlm-roberta-base: https://huggingface.co/xlm-roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16d8eedc-d75d-45b0-bec3-84df148e9841",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_1 = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "model_1 = AutoModelForMaskedLM.from_pretrained('xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ed21d5-faaf-4b50-ad28-8617a0820fd5",
   "metadata": {},
   "source": [
    "---\n",
    "xlm-roberta-large: https://huggingface.co/xlm-roberta-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7c4a7a3-5a36-4969-8eec-56a07190686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_2 = AutoTokenizer.from_pretrained('xlm-roberta-large')\n",
    "model_2 = AutoModelForMaskedLM.from_pretrained('xlm-roberta-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2f959c-697d-4e93-9b2e-e30affbb648d",
   "metadata": {},
   "source": [
    "---\n",
    "Twitter/twhin-bert-large: https://huggingface.co/Twitter/twhin-bert-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbd7c81f-8a5b-434b-baff-ae1e7082e4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_3 = AutoTokenizer.from_pretrained('Twitter/twhin-bert-large')\n",
    "model_3 = AutoModelForMaskedLM.from_pretrained('Twitter/twhin-bert-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37202fa3-5635-4e3d-917c-de297c4b0024",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9a831b5-4648-42e3-8ecc-28cbd8f36950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bold(phrase):\n",
    "    return '\\033[1m' + phrase + '\\033[0;0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0a99b8f-0c01-4ff8-afb4-1623deab47dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['xlm-roberta-base', 'xlm-roberta-large', 'twhin-bert-large']\n",
    "tokenizers = [tokenizer_1, tokenizer_2, tokenizer_3]\n",
    "models = [model_1, model_2, model_3]\n",
    "\n",
    "def test_models(sentence):\n",
    "    print(bold(sentence))\n",
    "    # I'm using [mask] instead of < mask > to avoid masking the text by CSS\n",
    "    sentence = sentence.replace('[mask]', '<mask>')\n",
    "    for name, tokenizer, model in zip(model_names, tokenizers, models):\n",
    "        inputs = tokenizer(sentence, return_tensors='pt')\n",
    "        mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
    "        \n",
    "        token_logits = model(**inputs).logits\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "        \n",
    "        top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "        print(f'{name:>20}: {[tokenizer.decode([token]) for token in top_5_tokens]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f643594-8022-487c-9571-5b7409268ebd",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "135bde62-404d-4108-964b-9b3d96733ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mMój [mask] jest najszybszy.\u001b[0;0m\n",
      "    xlm-roberta-base: ['świat', 'dzień', 'telefon', 'dom', 'czas']\n",
      "   xlm-roberta-large: ['komputer', 'blog', 'samochód', 'telefon', 'laptop']\n",
      "    twhin-bert-large: ['brat', 'chłopak', 'dom', 'dzień', 'chłop']\n",
      "==================================================================\n",
      "\u001b[1mNa świecie nie ma już żadnego [mask].\u001b[0;0m\n",
      "    xlm-roberta-base: ['Boga', 'człowieka', 'domu', 'państwa', 'pokoju']\n",
      "   xlm-roberta-large: ['porządku', 'problemu', 'prawa', 'człowieka', 'Boga']\n",
      "    twhin-bert-large: ['sensu', 'znaczenia', 'miejsca', 'kaca', 'Boga']\n",
      "==================================================================\n",
      "\u001b[1mMusiałem oddać pieniądze mojemu [mask].\u001b[0;0m\n",
      "    xlm-roberta-base: ['dziecku', 'domu', 'banku', '.', 'kolegi']\n",
      "   xlm-roberta-large: ['dziecku', 'domu', 'kolegi', 'banku', 'tatu']\n",
      "    twhin-bert-large: ['dziecku', 'kolegi', 'Bogu', 'mamy', 'pracy']\n",
      "==================================================================\n",
      "\u001b[1mTy to masz fajnego [mask].\u001b[0;0m\n",
      "    xlm-roberta-base: ['psa', 'bloga', '.', 'syna', 'człowieka']\n",
      "   xlm-roberta-large: ['psa', 'bloga', 'syna', 'kota', 'grafika']\n",
      "    twhin-bert-large: ['kaca', 'człowieka', 'dnia', 'syna', 'życia']\n",
      "==================================================================\n",
      "\u001b[1mWybrałem się na spacer z moim [mask].\u001b[0;0m\n",
      "    xlm-roberta-base: ['dzieckiem', 'samochodem', 'autem', 'kolegi', 'dziećmi']\n",
      "   xlm-roberta-large: ['dzieckiem', 'samochodem', 'autem', 'i', 'kolegi']\n",
      "    twhin-bert-large: ['dzieckiem', 'autem', 'i', 'dziećmi', 'samochodem']\n",
      "==================================================================\n",
      "\u001b[1mCzęsto opowiadam o moim [mask].\u001b[0;0m\n",
      "    xlm-roberta-base: ['życiu', 'świecie', 'blogu', 'hobby', 'przypadku']\n",
      "   xlm-roberta-large: ['życiu', 'domu', 'hobby', 'blogu', 'mieście']\n",
      "    twhin-bert-large: ['życiu', 'związku', 'ciele', 'dniu', 'stylu']\n",
      "==================================================================\n",
      "\u001b[1mZostaw mnie, ty [mask]!\u001b[0;0m\n",
      "    xlm-roberta-base: ['mój', 'ś', 'też', 'proszę', 'tule']\n",
      "   xlm-roberta-large: ['droga', 'moja', 'mój', 'idiot', 'moje']\n",
      "    twhin-bert-large: ['jesteś', 'wiesz', 'masz', 'ś', 'też']\n",
      "==================================================================\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    'Mój [mask] jest najszybszy.',\n",
    "    'Na świecie nie ma już żadnego [mask].',\n",
    "    'Musiałem oddać pieniądze mojemu [mask].',\n",
    "    'Ty to masz fajnego [mask].',\n",
    "    'Wybrałem się na spacer z moim [mask].',\n",
    "    'Często opowiadam o moim [mask].',\n",
    "    'Zostaw mnie, ty [mask]!'\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    test_models(sentence)\n",
    "    print('==================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ad784b-7d44-4455-a04e-012351708f90",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "568ffa09-f0ac-4935-9548-df6e0236e558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTa klawiatura, co mi ją dałeś do naprawy, to się jeszcze bardziej [mask].\u001b[0;0m\n",
      "    xlm-roberta-base: ['podoba', 'przyda', 'prezentuje', 'cieszy', 'nadaje']\n",
      "   xlm-roberta-large: ['przyda', 'podoba', 'sprawdza', 'stara', 'nadaje']\n",
      "    twhin-bert-large: ['podoba', 'trzyma', 'cieszy', 'przyda', 'boje']\n",
      "==================================================================\n",
      "\u001b[1mPamiętasz Monikę i jej wieczne problemy z alkoholem? Ostatnio [mask] do Stanów.\u001b[0;0m\n",
      "    xlm-roberta-base: ['wraca', 'wróci', 'trafił', 'podróż', 'chodzi']\n",
      "   xlm-roberta-large: ['wraca', 'jeździ', 'wróci', 'idzie', 'chodzi']\n",
      "    twhin-bert-large: ['wraca', 'trafił', 'wróci', 'została', 'idzie']\n",
      "==================================================================\n",
      "\u001b[1mCi bandyci ukradli mi składaka! Jak ja sobie bez [mask] poradzę.\u001b[0;0m\n",
      "    xlm-roberta-base: ['nich', 'niego', 'tego', 'problemu', 'głowy']\n",
      "   xlm-roberta-large: ['nich', 'tego', 'niego', 'ich', 'was']\n",
      "    twhin-bert-large: ['nich', 'tego', 'ich', 'was', 'niego']\n",
      "==================================================================\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    'Ta klawiatura, co mi ją dałeś do naprawy, to się jeszcze bardziej [mask].',\n",
    "    'Pamiętasz Monikę i jej wieczne problemy z alkoholem? Ostatnio [mask] do Stanów.',\n",
    "    'Ci bandyci ukradli mi składaka! Jak ja sobie bez [mask] poradzę.'\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    test_models(sentence)\n",
    "    print('==================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f7e38b-d757-47a9-b81f-7259e47ca3df",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fab3ace-8b85-48ee-a853-85e9e16784e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[mask] wrze w temperaturze 100 stopni.\u001b[0;0m\n",
      "    xlm-roberta-base: ['Można', '-', 'W', 'Na', 'Najlepiej']\n",
      "   xlm-roberta-large: ['Olej', '-', 'piekarnik', 'olej', 'Wszystko']\n",
      "    twhin-bert-large: ['Polska', 'Warszawa', 'Kraków', 'Poznań', 'Ukraina']\n",
      "==================================================================\n",
      "\u001b[1mWoda [mask] w temperaturze 100 stopni.\u001b[0;0m\n",
      "    xlm-roberta-base: [':', ',', 'wody', 'ciepła', '–']\n",
      "   xlm-roberta-large: ['jest', 'pozostaje', 'najlepiej', '-', 'zawsze']\n",
      "    twhin-bert-large: ['spada', 'rośnie', 'jest', 'ciepła', 'wisi']\n",
      "==================================================================\n",
      "\u001b[1m[mask] to najdłuższa rzeka w Polsce.\u001b[0;0m\n",
      "    xlm-roberta-base: ['Jest', 'jest', 'Polska', 'Obecnie', 'Będzie']\n",
      "   xlm-roberta-large: ['Jest', 'jest', 'Będzie', 'A', '-']\n",
      "    twhin-bert-large: ['Polska', 'Warszawa', 'Niemcy', 'Ukraina', 'Twitter']\n",
      "==================================================================\n",
      "\u001b[1mWisła to [mask] rzeka w Polsce.\u001b[0;0m\n",
      "    xlm-roberta-base: ['najlepsza', 'pierwsza', 'druga', 'główna', 'duża']\n",
      "   xlm-roberta-large: ['główna', 'najlepsza', 'pierwsza', 'druga', 'ogromna']\n",
      "    twhin-bert-large: ['najlepsza', 'nowa', 'pierwsza', 'piękna', 'taka']\n",
      "==================================================================\n",
      "\u001b[1mNie wolno korzystać z [mask] w trakcie jazdy.\u001b[0;0m\n",
      "    xlm-roberta-base: ['samochodu', 'pojazdów', 'urządzenia', 'urządzeń', 'samochodów']\n",
      "   xlm-roberta-large: ['telefonu', 'niej', 'niego', 'nich', 'internetu']\n",
      "    twhin-bert-large: ['telefonu', 'alkoholu', 'samochodu', 'światła', 'powietrza']\n",
      "==================================================================\n",
      "\u001b[1mRobert Makłowicz to [mask] człowiek na świecie.\u001b[0;0m\n",
      "    xlm-roberta-base: ['najlepszy', 'najważniejszy', 'największy', 'jedyny', 'pierwszy']\n",
      "   xlm-roberta-large: ['najlepszy', 'największy', 'najważniejszy', 'pierwszy', 'jedyny']\n",
      "    twhin-bert-large: ['najlepszy', 'najważniejszy', 'wspaniały', 'największy', 'wyjątkowy']\n",
      "==================================================================\n",
      "\u001b[1mAdolf Hitler to [mask] człowiek na świecie.\u001b[0;0m\n",
      "    xlm-roberta-base: ['najważniejszy', 'najlepszy', 'największy', 'pierwszy', 'jedyny']\n",
      "   xlm-roberta-large: ['najważniejszy', 'najlepszy', 'największy', 'pierwszy', 'najwyższy']\n",
      "    twhin-bert-large: ['najlepszy', 'najważniejszy', 'największy', 'jedyny', 'wspaniały']\n",
      "==================================================================\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    '[mask] wrze w temperaturze 100 stopni.',\n",
    "    'Woda [mask] w temperaturze 100 stopni.',\n",
    "    '[mask] to najdłuższa rzeka w Polsce.',\n",
    "    'Wisła to [mask] rzeka w Polsce.',\n",
    "    'Nie wolno korzystać z [mask] w trakcie jazdy.',\n",
    "    'Robert Makłowicz to [mask] człowiek na świecie.',\n",
    "    'Adolf Hitler to [mask] człowiek na świecie.'\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    test_models(sentence)\n",
    "    print('==================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ae49e1-ef82-4089-94cf-0c67d0e827f6",
   "metadata": {},
   "source": [
    "# Task 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def55d7e-a286-4541-9933-d0eae43a3763",
   "metadata": {},
   "source": [
    "#### Which of the models produced the best results?\n",
    "Although all the results seem similar, xlm-roberta-large produced the best results. Her superiority can be observed in the task with Polish cases – her predictions make  a bit more sense than the others. For instance:\n",
    "\n",
    "**Mój [mask] jest najszybszy**.\n",
    "- xlm-roberta-base: ['świat', 'dzień', 'telefon', 'dom', 'czas']\n",
    "- xlm-roberta-large: ['komputer', 'blog', 'samochód', 'telefon', 'laptop']\n",
    "- twhin-bert-large: ['brat', 'chłopak', 'dom', 'dzień', 'chłop']\n",
    "\n",
    "#### Was any of the models able to capture Polish grammar?\n",
    "All of them performed simiraly. They were able to perfectly capture all the cases except for:\n",
    "- dative (the first predictions (\"dziecku\") is fine, but we can find words like \"tatu\", \"kolegi\", or \"mamy\" with smaller probabilities), and\n",
    "- vocative (none of the models managed to predict at least one sensible word; xlm-roberta-large with ['droga', 'moja', 'mój', 'idiot', 'moje'] was the closest one). \n",
    "\n",
    "#### Was any of the models able to capture long-distant relationships between the words?\n",
    "It depends on the sentence.\n",
    "1. In the first sentence, all the models performed good.\n",
    "2. In the second sentence, each model was quite fine, but only xlm-roberta-large made no errors (at least when it comes to the feminine form).\n",
    "3. The models failed in the third sentence by pointing towards \"bandyci\" instead of \"składaka\". Each of them managed to predict the correct answet (\"niego\"), but not with the highest probability.\n",
    "\n",
    "#### Was any of the models able to capture world knowledge?\n",
    "- All the models failed in the real world knowledge sentences. xlm-roberta-large seems to be the best in the boiling water sentences. The answers in the sentences about Vistula make some sense, but none of the models managed to predict \"najdłuższa\".\n",
    "- The models performed better in the 5th sentence about the general law present around the world (maybe except for xlm-roberta-base: \"Nie wolno używać samochodu w trakcie jazdy\" as the first prediction).\n",
    "- It seems that, for the models, every person around the world is the best. While in the case of Robert Makłowicz this is true, it is awfully incorrect in the second case - Adolf H. (coaching level hard).\n",
    "\n",
    "#### What are the most striking errors made by the models?\n",
    "- Definitely most striking error: predicting Adolf Hitler as the best person on the world.\n",
    "- twinh-bert-large in the third problem: putting proper nouns at the beginning of a sentence (in both cases), e.g. \"[Polska/Twitter/Ukraina] to najdłuższa rzeka w Polsce\".\n",
    "- xlm-roberta-base: Spamming with punctuation marks in the second sentence, third problem.\n",
    "- \"Ci bandyci ukradli mi składaka! Jak ja sobie bez *głowy* poradzę.\"\n",
    "- \"Zostaw mnie, ty *ś*\", \"Zostaw mnie, ty tule\" (tytule -> ty tule?)\n",
    "- \"Ty to masz fajnego *.*\" (period)\n",
    "- \"Musiałem oddać pieniądze mojemu *tatu*\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f054742-46fe-4b15-a62d-787d5a4fa79b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
