{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82f83eed-555f-4288-a78e-ffa082b7c621",
   "metadata": {},
   "source": [
    "# Przetwarzanie języka naturalnego – lab4\n",
    "## Mateusz Kocot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5636afe-a8a1-442a-ae00-99cb6509f9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b47ac6-3ff5-41fe-a494-90d4a55290ec",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10d0ca8-be56-4cfa-9168-d4ac24d9818b",
   "metadata": {},
   "source": [
    "xlm-roberta-base: https://huggingface.co/xlm-roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16d8eedc-d75d-45b0-bec3-84df148e9841",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_1 = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "model_1 = AutoModelForMaskedLM.from_pretrained('xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ed21d5-faaf-4b50-ad28-8617a0820fd5",
   "metadata": {},
   "source": [
    "---\n",
    "xlm-roberta-large: https://huggingface.co/xlm-roberta-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7c4a7a3-5a36-4969-8eec-56a07190686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_2 = AutoTokenizer.from_pretrained('xlm-roberta-large')\n",
    "model_2 = AutoModelForMaskedLM.from_pretrained('xlm-roberta-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2f959c-697d-4e93-9b2e-e30affbb648d",
   "metadata": {},
   "source": [
    "---\n",
    "Twitter/twhin-bert-large: https://huggingface.co/Twitter/twhin-bert-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbd7c81f-8a5b-434b-baff-ae1e7082e4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_3 = AutoTokenizer.from_pretrained('Twitter/twhin-bert-large')\n",
    "model_3 = AutoModelForMaskedLM.from_pretrained('Twitter/twhin-bert-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37202fa3-5635-4e3d-917c-de297c4b0024",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9a831b5-4648-42e3-8ecc-28cbd8f36950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bold(phrase):\n",
    "    return '\\033[1m' + phrase + '\\033[0;0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0a99b8f-0c01-4ff8-afb4-1623deab47dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['xlm-roberta-base', 'xlm-roberta-large', 'twhin-bert-large']\n",
    "tokenizers = [tokenizer_1, tokenizer_2, tokenizer_3]\n",
    "models = [model_1, model_2, model_3]\n",
    "\n",
    "def test_models(sentence):\n",
    "    print(bold(sentence))\n",
    "    for name, tokenizer, model in zip(model_names, tokenizers, models):\n",
    "        inputs = tokenizer(sentence, return_tensors='pt')\n",
    "        mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
    "        \n",
    "        token_logits = model(**inputs).logits\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "        \n",
    "        top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "        print(f'{name:>20}: {[tokenizer.decode([token]) for token in top_5_tokens]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f643594-8022-487c-9571-5b7409268ebd",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "135bde62-404d-4108-964b-9b3d96733ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mMój <mask> jest najszybszy.\u001b[0;0m\n",
      "    xlm-roberta-base: ['świat', 'dzień', 'telefon', 'dom', 'czas']\n",
      "   xlm-roberta-large: ['komputer', 'blog', 'samochód', 'telefon', 'laptop']\n",
      "    twhin-bert-large: ['brat', 'chłopak', 'dom', 'dzień', 'chłop']\n",
      "==================================================================\n",
      "\u001b[1mNa świecie nie ma już żadnego <mask>.\u001b[0;0m\n",
      "    xlm-roberta-base: ['Boga', 'człowieka', 'domu', 'państwa', 'pokoju']\n",
      "   xlm-roberta-large: ['porządku', 'problemu', 'prawa', 'człowieka', 'Boga']\n",
      "    twhin-bert-large: ['sensu', 'znaczenia', 'miejsca', 'kaca', 'Boga']\n",
      "==================================================================\n",
      "\u001b[1mMusiałem oddać pieniądze mojemu <mask>.\u001b[0;0m\n",
      "    xlm-roberta-base: ['dziecku', 'domu', 'banku', '.', 'kolegi']\n",
      "   xlm-roberta-large: ['dziecku', 'domu', 'kolegi', 'banku', 'tatu']\n",
      "    twhin-bert-large: ['dziecku', 'kolegi', 'Bogu', 'mamy', 'pracy']\n",
      "==================================================================\n",
      "\u001b[1mTy to masz fajnego <mask>.\u001b[0;0m\n",
      "    xlm-roberta-base: ['psa', 'bloga', '.', 'syna', 'człowieka']\n",
      "   xlm-roberta-large: ['psa', 'bloga', 'syna', 'kota', 'grafika']\n",
      "    twhin-bert-large: ['kaca', 'człowieka', 'dnia', 'syna', 'życia']\n",
      "==================================================================\n",
      "\u001b[1mWybrałem się na spacer z moim <mask>.\u001b[0;0m\n",
      "    xlm-roberta-base: ['dzieckiem', 'samochodem', 'autem', 'kolegi', 'dziećmi']\n",
      "   xlm-roberta-large: ['dzieckiem', 'samochodem', 'autem', 'i', 'kolegi']\n",
      "    twhin-bert-large: ['dzieckiem', 'autem', 'i', 'dziećmi', 'samochodem']\n",
      "==================================================================\n",
      "\u001b[1mCzęsto opowiadam o moim <mask>.\u001b[0;0m\n",
      "    xlm-roberta-base: ['życiu', 'świecie', 'blogu', 'hobby', 'przypadku']\n",
      "   xlm-roberta-large: ['życiu', 'domu', 'hobby', 'blogu', 'mieście']\n",
      "    twhin-bert-large: ['życiu', 'związku', 'ciele', 'dniu', 'stylu']\n",
      "==================================================================\n",
      "\u001b[1mZostaw mnie, ty <mask>!\u001b[0;0m\n",
      "    xlm-roberta-base: ['mój', 'ś', 'też', 'proszę', 'tule']\n",
      "   xlm-roberta-large: ['droga', 'moja', 'mój', 'idiot', 'moje']\n",
      "    twhin-bert-large: ['jesteś', 'wiesz', 'masz', 'ś', 'też']\n",
      "==================================================================\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    'Mój <mask> jest najszybszy.',\n",
    "    'Na świecie nie ma już żadnego <mask>.',\n",
    "    'Musiałem oddać pieniądze mojemu <mask>.',\n",
    "    'Ty to masz fajnego <mask>.',\n",
    "    'Wybrałem się na spacer z moim <mask>.',\n",
    "    'Często opowiadam o moim <mask>.',\n",
    "    'Zostaw mnie, ty <mask>!'\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    test_models(sentence)\n",
    "    print('==================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ad784b-7d44-4455-a04e-012351708f90",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "568ffa09-f0ac-4935-9548-df6e0236e558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTa klawiatura, co mi ją dałeś do naprawy, to się jeszcze bardziej <mask>.\u001b[0;0m\n",
      "    xlm-roberta-base: ['podoba', 'przyda', 'prezentuje', 'cieszy', 'nadaje']\n",
      "   xlm-roberta-large: ['przyda', 'podoba', 'sprawdza', 'stara', 'nadaje']\n",
      "    twhin-bert-large: ['podoba', 'trzyma', 'cieszy', 'przyda', 'boje']\n",
      "==================================================================\n",
      "\u001b[1mPamiętasz Monikę i jej wieczne problemy z alkoholem? Ostatnio <mask> do Stanów.\u001b[0;0m\n",
      "    xlm-roberta-base: ['wraca', 'wróci', 'trafił', 'podróż', 'chodzi']\n",
      "   xlm-roberta-large: ['wraca', 'jeździ', 'wróci', 'idzie', 'chodzi']\n",
      "    twhin-bert-large: ['wraca', 'trafił', 'wróci', 'została', 'idzie']\n",
      "==================================================================\n",
      "\u001b[1mCi bandyci ukradli mi składaka! Jak ja sobie bez <mask> poradzę.\u001b[0;0m\n",
      "    xlm-roberta-base: ['nich', 'niego', 'tego', 'problemu', 'głowy']\n",
      "   xlm-roberta-large: ['nich', 'tego', 'niego', 'ich', 'was']\n",
      "    twhin-bert-large: ['nich', 'tego', 'ich', 'was', 'niego']\n",
      "==================================================================\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    'Ta klawiatura, co mi ją dałeś do naprawy, to się jeszcze bardziej <mask>.',\n",
    "    'Pamiętasz Monikę i jej wieczne problemy z alkoholem? Ostatnio <mask> do Stanów.',\n",
    "    'Ci bandyci ukradli mi składaka! Jak ja sobie bez <mask> poradzę.'\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    test_models(sentence)\n",
    "    print('==================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f7e38b-d757-47a9-b81f-7259e47ca3df",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fab3ace-8b85-48ee-a853-85e9e16784e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m<mask> wrze w temperaturze 100 stopni.\u001b[0;0m\n",
      "    xlm-roberta-base: ['Można', '-', 'W', 'Na', 'Najlepiej']\n",
      "   xlm-roberta-large: ['Olej', '-', 'piekarnik', 'olej', 'Wszystko']\n",
      "    twhin-bert-large: ['Polska', 'Warszawa', 'Kraków', 'Poznań', 'Ukraina']\n",
      "==================================================================\n",
      "\u001b[1mWoda <mask> w temperaturze 100 stopni.\u001b[0;0m\n",
      "    xlm-roberta-base: [':', ',', 'wody', 'ciepła', '–']\n",
      "   xlm-roberta-large: ['jest', 'pozostaje', 'najlepiej', '-', 'zawsze']\n",
      "    twhin-bert-large: ['spada', 'rośnie', 'jest', 'ciepła', 'wisi']\n",
      "==================================================================\n",
      "\u001b[1m<mask> to najdłuższa rzeka w Polsce.\u001b[0;0m\n",
      "    xlm-roberta-base: ['Jest', 'jest', 'Polska', 'Obecnie', 'Będzie']\n",
      "   xlm-roberta-large: ['Jest', 'jest', 'Będzie', 'A', '-']\n",
      "    twhin-bert-large: ['Polska', 'Warszawa', 'Niemcy', 'Ukraina', 'Twitter']\n",
      "==================================================================\n",
      "\u001b[1mWisła to <mask> rzeka w Polsce.\u001b[0;0m\n",
      "    xlm-roberta-base: ['najlepsza', 'pierwsza', 'druga', 'główna', 'duża']\n",
      "   xlm-roberta-large: ['główna', 'najlepsza', 'pierwsza', 'druga', 'ogromna']\n",
      "    twhin-bert-large: ['najlepsza', 'nowa', 'pierwsza', 'piękna', 'taka']\n",
      "==================================================================\n",
      "\u001b[1mNie wolno korzystać z <mask> w trakcie jazdy.\u001b[0;0m\n",
      "    xlm-roberta-base: ['samochodu', 'pojazdów', 'urządzenia', 'urządzeń', 'samochodów']\n",
      "   xlm-roberta-large: ['telefonu', 'niej', 'niego', 'nich', 'internetu']\n",
      "    twhin-bert-large: ['telefonu', 'alkoholu', 'samochodu', 'światła', 'powietrza']\n",
      "==================================================================\n",
      "\u001b[1mRobert Makłowicz to <mask> człowiek na świecie.\u001b[0;0m\n",
      "    xlm-roberta-base: ['najlepszy', 'najważniejszy', 'największy', 'jedyny', 'pierwszy']\n",
      "   xlm-roberta-large: ['najlepszy', 'największy', 'najważniejszy', 'pierwszy', 'jedyny']\n",
      "    twhin-bert-large: ['najlepszy', 'najważniejszy', 'wspaniały', 'największy', 'wyjątkowy']\n",
      "==================================================================\n",
      "\u001b[1mAdolf Hitler to <mask> człowiek na świecie.\u001b[0;0m\n",
      "    xlm-roberta-base: ['najważniejszy', 'najlepszy', 'największy', 'pierwszy', 'jedyny']\n",
      "   xlm-roberta-large: ['najważniejszy', 'najlepszy', 'największy', 'pierwszy', 'najwyższy']\n",
      "    twhin-bert-large: ['najlepszy', 'najważniejszy', 'największy', 'jedyny', 'wspaniały']\n",
      "==================================================================\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    '<mask> wrze w temperaturze 100 stopni.',\n",
    "    'Woda <mask> w temperaturze 100 stopni.',\n",
    "    '<mask> to najdłuższa rzeka w Polsce.',\n",
    "    'Wisła to <mask> rzeka w Polsce.',\n",
    "    'Nie wolno korzystać z <mask> w trakcie jazdy.',\n",
    "    'Robert Makłowicz to <mask> człowiek na świecie.',\n",
    "    'Adolf Hitler to <mask> człowiek na świecie.'\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    test_models(sentence)\n",
    "    print('==================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdeab10-2eb7-41b6-b1cb-82766d98435f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
