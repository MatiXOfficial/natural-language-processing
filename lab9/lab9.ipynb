{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f9e1a58-7ec7-40c7-98cb-bdb7ccc5fc3f",
   "metadata": {},
   "source": [
    "# Przetwarzanie języka naturalnego – lab9\n",
    "## Mateusz Kocot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd5823f9-bded-495f-9228-9dbe07e7be67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "from pprint import pprint\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict, load_dataset, concatenate_datasets\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33c3acaf-face-44e5-8109-827bd49aadec",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTIONS_PATH = '../../simple-legal-questions-pl/questions.jl'\n",
    "RELEVANT_PATH = '../../simple-legal-questions-pl/relevant.jl'\n",
    "PASSAGES_PATH ='../../simple-legal-questions-pl/passages.jl'\n",
    "ANSWERS_PATH = '../../simple-legal-questions-pl/answers.jl'\n",
    "\n",
    "TEST_RANGE = (1345, 1366) # left: inclusive, right: exclusive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9744f4-a9da-4e8b-a830-0121d3d6ce66",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "- issue: https://github.com/apohllo/simple-legal-questions-pl/issues/96\n",
    "- PR: https://github.com/apohllo/simple-legal-questions-pl/pull/118"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a42f08-3814-408c-a51f-fc35175067f1",
   "metadata": {},
   "source": [
    "# Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abb4a3f1-e29b-4c92-a0a1-ba19b165b0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jl(path, dict_key=None, keys_to_delete=[]):\n",
    "    with open(path, encoding='utf-8') as file:\n",
    "        lines = [json.loads(line) for line in file]\n",
    "\n",
    "    if dict_key is not None:\n",
    "        lines_dict = {}\n",
    "        for line in lines:\n",
    "            key = line[dict_key]\n",
    "            del line[dict_key]\n",
    "            lines_dict[key] = line\n",
    "        lines = lines_dict\n",
    "        \n",
    "    if keys_to_delete:\n",
    "        lines_it = lines.values() if type(lines) == dict else lines\n",
    "        for line in lines_it:\n",
    "            for key in keys_to_delete:\n",
    "                del line[key]\n",
    "        \n",
    "    return lines\n",
    "    \n",
    "questions_jl = load_jl(QUESTIONS_PATH, dict_key='_id')\n",
    "relevant_jl = load_jl(RELEVANT_PATH, dict_key='question-id', keys_to_delete=['score'])\n",
    "passages_jl = load_jl(PASSAGES_PATH, dict_key='_id')\n",
    "answers_jl = load_jl(ANSWERS_PATH, dict_key='question-id', keys_to_delete=['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a929116-6305-4522-8c4b-9127288c1f29",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Question:\n",
    "    id_: str\n",
    "    question_text: str\n",
    "    passage: str\n",
    "    answer: str\n",
    "    title: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50ecefe3-fed7-41b8-b38e-b1a4fcf3b15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_dataset = {}\n",
    "for id_, question_dict in questions_jl.items():\n",
    "    # do not process if there is no answer or the answer is empty\n",
    "    if id_ not in answers_jl or 'answer' not in answers_jl[id_] or not answers_jl[id_]['answer']:\n",
    "        continue\n",
    "\n",
    "    question_text = question_dict['text']\n",
    "    passage_id = relevant_jl[id_]['passage-id']\n",
    "    passage = passages_jl[passage_id]['text']\n",
    "    answer = answers_jl[id_]['answer']\n",
    "    title = passages_jl[passage_id]['title']\n",
    "    \n",
    "    question = Question(id_, question_text, passage, answer, title)\n",
    "    questions_dataset[id_] = question\n",
    "    \n",
    "len(questions_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a29f9f-9ef8-4d6e-b0c3-a5052f3390f6",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c510016a-0ad8-4168-b232-06e922cef850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 19)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_test = [questions_dataset[str(id_)] for id_ in range(*TEST_RANGE)]\n",
    "questions_test_set = {q.question_text for q in questions_test}\n",
    "\n",
    "len(questions_test), len(questions_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a08c9e2f-94fa-4ad1-a83f-5832829628f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_pretrain = {}\n",
    "for id_, question in questions_dataset.items():\n",
    "    if question.question_text not in questions_test_set:\n",
    "        questions_pretrain[id_] = question\n",
    "        \n",
    "len(questions_pretrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c245ee-d933-4b22-a05f-209065ca2338",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b76a12fc-f7f8-4b93-9838-ef7a7fdd766b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 44)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(421)\n",
    "questions_val = list(questions_pretrain.values())\n",
    "random.shuffle(questions_val)\n",
    "\n",
    "l = int(0.2 * len(questions_pretrain))\n",
    "questions_val, _ = questions_val[:l], questions_val[l:]\n",
    "\n",
    "questions_val_set = {q.question_text for q in questions_val}\n",
    "\n",
    "len(questions_val), len(questions_val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63b46122-a37e-4b09-b141-34a859e292a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_train = {}\n",
    "for id_, question in questions_pretrain.items():\n",
    "    if question.question_text not in questions_val_set:\n",
    "        questions_train[id_] = question\n",
    "        \n",
    "questions_train = list(questions_train.values())\n",
    "len(questions_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e237432-35d5-4445-94b5-a3ac452e18de",
   "metadata": {},
   "source": [
    "# Task 5\n",
    "Use 2000 questions from SQUAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9631a5c9-00e1-43b3-a1c1-efc106d391f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (C:/Users/MatiX/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28c6a8eb4d646f98293300cb3668e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "squad_dataset = load_dataset('squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbd79719-9cb0-4276-97eb-c634bc2bb19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dfe8b63-80c8-4788-b197-39ba62415e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 2000\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset_train = Dataset.from_dict(squad_dataset['train'][:2000]) # take only 2000\n",
    "squad_dataset_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c462a0-a8ec-4117-85f1-b764815cd5b8",
   "metadata": {},
   "source": [
    "# Transform to the squad format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3b6b502-1aec-4632-b81b-a929d29949b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_squad(dataset: list[Question]):\n",
    "    result = {\n",
    "        'answers': [],\n",
    "        'context': [],\n",
    "        'id': [],\n",
    "        'question': [],\n",
    "        'title': []\n",
    "    }\n",
    "    for question in dataset:\n",
    "        result['answers'].append({\n",
    "            'answer_start': [0], # not provided in the dataset\n",
    "            'text': [question.answer]\n",
    "        })\n",
    "        result['context'].append(question.passage)\n",
    "        result['id'].append(question.id_)\n",
    "        result['question'].append(question.question_text)\n",
    "        result['title'].append(question.title)\n",
    "    return Dataset.from_dict(result)\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    'train': concatenate_datasets([to_squad(questions_train), squad_dataset_train]),\n",
    "    'validation': to_squad(questions_val),\n",
    "    'test': to_squad(questions_test)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c30002b9-18b3-43ad-b33e-1951a0fcb762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
       "        num_rows: 2167\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
       "        num_rows: 48\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['answers', 'context', 'id', 'question', 'title'],\n",
       "        num_rows: 21\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c3f1834-8378-4a71-b02c-b308adced407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['answers', 'context', 'id', 'question', 'title'],\n",
      "    num_rows: 2167\n",
      "})\n",
      "{'answers': {'answer_start': [0],\n",
      "             'text': ['postępowanie wszczęte w wyniku dokonania zgłoszenia '\n",
      "                      'lub złożenia wniosku podlega umorzeniu, bądź '\n",
      "                      'czynność uzależniona od opłaty zostaje zaniechana.']},\n",
      " 'context': 'Art. 223. 1. Opłaty jednorazowe za zgłoszenia, wnioski, '\n",
      "            'oświadczenia i inne czynności przewidziane w ustawie powinny być '\n",
      "            'uiszczane z góry, o ile ustawa lub rozporządzenie, o którym mowa '\n",
      "            'w art. 222 ust. 3, nie przewiduje uiszczenia opłaty na wezwanie '\n",
      "            'Urzędu Patentowego w określonym terminie. 2. Opłata jednorazowa '\n",
      "            'za zgłoszenie może być również uiszczona w ciągu jednego miesiąca '\n",
      "            'od daty doręczenia wezwania Urzędu Patentowego. 3. Jeżeli w '\n",
      "            'wyniku złożonego wniosku o ponowne rozpatrzenie sprawy decyzja '\n",
      "            'lub postanowienie Urzędu Patentowego zostało uchylone, opłata '\n",
      "            'uiszczona od tego wniosku podlega zwrotowi. 4. W razie '\n",
      "            'nieuiszczenia w terminie opłaty, o której mowa w ust. 1, '\n",
      "            'postępowanie wszczęte w wyniku dokonania zgłoszenia lub złożenia '\n",
      "            'wniosku podlega umorzeniu, bądź czynność uzależniona od opłaty '\n",
      "            'zostaje zaniechana.',\n",
      " 'id': '22',\n",
      " 'question': 'Co się stanie jeżeli nie zostanie uiszczona opłata dla wniosku '\n",
      "             'patentowego?',\n",
      " 'title': 'Ustawa z dnia 30 czerwca 2000 r. Prawo własności przemysłowej Tytuł '\n",
      "          'I Przepisy ogólne'}\n"
     ]
    }
   ],
   "source": [
    "print(datasets['train'])\n",
    "pprint(datasets['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9990556a-b7b5-4c31-9ad6-845770011c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2167 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/48 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/21 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets.save_to_disk('data/datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fc701b-128f-4cf0-85d8-66cf89e597a5",
   "metadata": {},
   "source": [
    "# Task 6\n",
    "Changes in *run_seq2seq_qa.py*:\n",
    "- Replaced `load_dataset` with `load_from_disk`.\n",
    "\n",
    "Used three models: allegro/plt5-small, allegro/plt5-base and google/mt5-small (larger models did not fit into my 8GB VRAM). Fine-tuning with following commands.\n",
    "\n",
    "```bash\n",
    "python src/run_seq2seq_qa.py \\\n",
    "    --model_name_or_path allegro/plt5-small \\\n",
    "    --dataset_name data/datasets \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --do_predict \\\n",
    "    --predict_with_generate \\\n",
    "    --per_device_train_batch_size 8 \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --output_dir ./output/allegro_small/ \\\n",
    "    --overwrite_output_dir\n",
    "```\n",
    "\n",
    "```bash\n",
    "python src/run_seq2seq_qa.py \\\n",
    "    --model_name_or_path allegro/plt5-base \\\n",
    "    --dataset_name data/datasets \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --do_predict \\\n",
    "    --predict_with_generate \\\n",
    "    --per_device_train_batch_size 3 \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --output_dir ./output/allegro_base/ \\\n",
    "    --overwrite_output_dir\n",
    "```\n",
    "\n",
    "```bash\n",
    "python src/run_seq2seq_qa.py \\\n",
    "    --model_name_or_path google/mt5-small \\\n",
    "    --dataset_name data/datasets \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --do_predict \\\n",
    "    --predict_with_generate \\\n",
    "    --per_device_train_batch_size 6 \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --output_dir ./output/google_small/ \\\n",
    "    --overwrite_output_dir\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a4f2ac-0f78-4808-975f-9b18acfcf190",
   "metadata": {},
   "source": [
    "# Task 7 & 8\n",
    "- best results with allegro_base (allegro/plt5-base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0e27439-75e5-4b89-bcdc-0fdcdd457206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_exact_match</th>\n",
       "      <th>val_f1</th>\n",
       "      <th>test_exact_match</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>allegro_small</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.003588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.776366</td>\n",
       "      <td>8.58 min</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allegro_base</th>\n",
       "      <td>8.333333</td>\n",
       "      <td>27.530104</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>55.650237</td>\n",
       "      <td>41.35 min</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>google_small</th>\n",
       "      <td>2.083333</td>\n",
       "      <td>15.873639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.841166</td>\n",
       "      <td>18.00 min</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               val_exact_match     val_f1  test_exact_match    test_f1  \\\n",
       "allegro_small         0.000000   9.003588          0.000000  10.776366   \n",
       "allegro_base          8.333333  27.530104         42.857143  55.650237   \n",
       "google_small          2.083333  15.873639          0.000000   8.841166   \n",
       "\n",
       "              train_runtime  epochs  \n",
       "allegro_small      8.58 min      10  \n",
       "allegro_base      41.35 min      10  \n",
       "google_small      18.00 min      10  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_paths = ['output/allegro_small', 'output/allegro_base', 'output/google_small']\n",
    "model_names = ['allegro_small', 'allegro_base', 'google_small']\n",
    "\n",
    "eval_exact_match = []\n",
    "eval_f1 = []\n",
    "test_exact_match = []\n",
    "test_f1 = []\n",
    "train_runtime = []\n",
    "epochs = []\n",
    "\n",
    "for path, model_name in zip(model_paths, model_names):\n",
    "    with open(path + '/all_results.json', encoding='utf-8') as file:\n",
    "        results = json.load(file)\n",
    "    eval_exact_match.append(results['eval_exact_match'])\n",
    "    eval_f1.append(results['eval_f1'])\n",
    "    test_exact_match.append(results['test_exact_match'])\n",
    "    test_f1.append(results['test_f1'])\n",
    "    train_runtime.append(f\"{int(results['train_runtime']) / 60:0.2f} min\")\n",
    "    epochs.append(int(results['epoch']))\n",
    "    \n",
    "pd.DataFrame({\n",
    "    'val_exact_match': eval_exact_match, \n",
    "    'val_f1': eval_f1, \n",
    "    'test_exact_match': test_exact_match,\n",
    "    'test_f1': test_f1, \n",
    "    'train_runtime': train_runtime,\n",
    "    'epochs': epochs\n",
    "}, index=model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06e1ec7-1860-479d-a562-9c93b8adaa19",
   "metadata": {},
   "source": [
    "# Task 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "161f7394-3cdf-48c8-bbf9-a533b071e4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = 'output/allegro_base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(best_model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39e046e5-91c6-412d-90fe-b9b747db4d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answer(question, context):\n",
    "    inputs = tokenizer(f\"question: {question} context: {context}\", return_tensors='pt')\n",
    "    outputs = model.generate(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fc8a73d-1518-4085-841f-0b186b476f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Question: \u001b[1mCzy źródła finansowania partii politycznych mogą być niejawne?\u001b[0;0m\n",
      "Expected answer: Źródła finansowania partii politycznych są jawne.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MatiX\\anaconda3\\envs\\nlp-py39\\lib\\site-packages\\transformers\\generation\\utils.py:1234: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returned answer: Państwowa Komisja Wyborcza\n",
      "=========================================================================================\n",
      "       Question: \u001b[1mCzy wnuki przyjęte na wychowanie mają prawo do renty po zmarłych rodzicach?\u001b[0;0m\n",
      "Expected answer: Nie\n",
      "Returned answer: nie\n",
      "=========================================================================================\n",
      "       Question: \u001b[1mKiedy kurator składa wniosek o ogłoszenie upadłości osoby prawnej?\u001b[0;0m\n",
      "Expected answer: jeżeli stwierdzi, że istnieją podstawy do ogłoszenia upadłości.\n",
      "Returned answer: Stwierdzi, że istnieją podstawy do ogłoszenia upadłości.\n",
      "=========================================================================================\n",
      "       Question: \u001b[1mCzy dawka promieniowania jonizującego pochodzącego ze źródeł naturalnych może przekroczyć dawkę graniczną?\u001b[0;0m\n",
      "Expected answer: Nie\n",
      "Returned answer: sztuczne źródła promieniowania jonizującego\n",
      "=========================================================================================\n",
      "       Question: \u001b[1mCzy dawka promieniowania jonizującego pochodzącego ze źródeł naturalnych może przekroczyć dawkę graniczną?\u001b[0;0m\n",
      "Expected answer: Nie\n",
      "Returned answer: Suma dawek promieniowania jonizującego pochodzącego ze źródeł naturalnych nie może przekroczyć dawki granicznej określonej\n",
      "=========================================================================================\n",
      "       Question: \u001b[1mJaki tytuł ma obecnie Wyższa Szkoła Inżynierska w Koszalinie?\u001b[0;0m\n",
      "Expected answer: Politechnika Koszalińska\n",
      "Returned answer: Politechnika Koszalińska\n",
      "=========================================================================================\n",
      "       Question: \u001b[1mCzy wydanie dyspozycji wyjazdu/wylotu zespołu ratownictwa medycznego w razie konieczności użycia dodatkowych jednostek systemu rodzi roszczenia finansowe?\u001b[0;0m\n",
      "Expected answer: Nie\n",
      "Returned answer: nie\n",
      "=========================================================================================\n",
      "       Question: \u001b[1mCo kontroluje okręgowa komisja rewizyjna?\u001b[0;0m\n",
      "Expected answer: działalność statutową, finansową i gospodarczą okręgowej izby\n",
      "Returned answer: działalność statutową, finansową i gospodarczą okręgowej izby\n",
      "=========================================================================================\n",
      "       Question: \u001b[1mCzy ktoś, kto przewozi materiał jądrowy bez właściwego zabezpeczenia podlega karze ograniczenia wolności?\u001b[0;0m\n",
      "Expected answer: Tak\n",
      "Returned answer: Od 3 miesięcy do lat 5\n",
      "=========================================================================================\n",
      "       Question: \u001b[1mCzy producent butelek miarowych może przekazać opis przyjętego systemu kontroli wewnętrznej po rozpoczęciu produkcji?\u001b[0;0m\n",
      "Expected answer: Nie\n",
      "Returned answer: dyrektorowi właściwego terytorialnie okręgowego urzędu miar\n",
      "=========================================================================================\n",
      "       Question: \u001b[1mCzy wyborca niepełnosprawny jest dopisywany do spisu wyborców?\u001b[0;0m\n",
      "Expected answer: Tak\n",
      "Returned answer: na wniosek wniesiony do urzędu gminy najpóźniej w 10 dniu przed dniem wyborów\n",
      "=========================================================================================\n",
      "       Question: \u001b[1mKto wydaje decyzje w sprawach udzielania i pozbawiania azylu w RP?\u001b[0;0m\n",
      "Expected answer: Minister Spraw Wewnętrznych i Administracji w porozumieniu z Ministrem Spraw Zagranicznych\n",
      "Returned answer: Minister Spraw Wewnętrznych i Administracji w porozumieniu z Ministrem Spraw Zagranicznych\n",
      "=========================================================================================\n",
      "       Question: \u001b[1mKto wydaje decyzje w sprawach udzielania i pozbawiania azylu w RP?\u001b[0;0m\n",
      "Expected answer: Minister Spraw Wewnętrznych i Administracji\n",
      "Returned answer: Minister Spraw Wewnętrznych i Administracji\n",
      "=========================================================================================\n",
      "       Question: \u001b[1mJak wysoka jest kara dla uchylającego się od służby funkcjonariusza BOR?\u001b[0;0m\n",
      "Expected answer: od 3 miesięcy do lat 5\n",
      "Returned answer: Od 3 miesięcy do lat 5\n",
      "=========================================================================================\n",
      "       Question: \u001b[1mNa czyj wniosek inne jednostki organizacyjne mogą uzyskać osobowość prawną?\u001b[0;0m\n",
      "Expected answer: Ministra Spraw Wewnętrznych i Administracji\n",
      "Returned answer: Zarząd Związku Gmin\n",
      "=========================================================================================\n",
      "       Question: \u001b[1mCzy strona może cofnąć odwołanie przed wydaniem decyzji przez organ odwoławczy?\u001b[0;0m\n",
      "Expected answer: Tak\n",
      "Returned answer: Czy strona może cofnąć odwołanie przed wydaniem decyzji przez organ odwoławczy\n",
      "=========================================================================================\n",
      "       Question: \u001b[1mKto może określić, w drodze rozporządzenia, dodatkowe kryteria akredytacyjne?\u001b[0;0m\n",
      "Expected answer: Minister właściwy do spraw finansów publicznych\n",
      "Returned answer: Minister właściwy do spraw finansów publicznych\n",
      "=========================================================================================\n",
      "       Question: \u001b[1mNa kim spoczywa obowiązek prowadzenia kontroli źródeł promieniowania jonizującego?\u001b[0;0m\n",
      "Expected answer: na kierowniku jednostki organizacyjnej wykonującej działalność związaną z tymi źródłami\n",
      "Returned answer: kierownik jednostki organizacyjnej wykonującej działalność związaną z tymi źródłami\n",
      "=========================================================================================\n",
      "       Question: \u001b[1mCo może ustalić organ ochrony środowiska w drodze decyzji?\u001b[0;0m\n",
      "Expected answer: wymagania w zakresie ochrony środowiska dotyczące eksploatacji instalacji, z której emisja nie wymaga pozwolenia, o ile jest to uzasadnione koniecznością ochrony środowiska\n",
      "Returned answer: wymagania w zakresie ochrony środowiska dotyczące eksploatacji instalacji, z której emisja nie wymaga pozwolenia\n",
      "=========================================================================================\n",
      "       Question: \u001b[1mKto określi w drodze rozporządzenia rodzaje preparatów i ich ilości jakie mogą posiadać zakłady opieki?\u001b[0;0m\n",
      "Expected answer: Minister Zdrowia i Opieki Społecznej\n",
      "Returned answer: Minister Zdrowia i Opieki Społecznej\n",
      "=========================================================================================\n",
      "       Question: \u001b[1mKto wykonuje zadania wojewody w zakresie ochrony przyrody na terenie parku narodowego?\u001b[0;0m\n",
      "Expected answer: dyrektor tego parku\n",
      "Returned answer: dyrektor tego parku\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "def bold(phrase):\n",
    "    return '\\033[1m' + phrase + '\\033[0;0m'\n",
    "\n",
    "for d in datasets['test']:\n",
    "    print('       Question:', bold(d['question']))\n",
    "    print('Expected answer:', d['answers']['text'][0])\n",
    "    answer = predict_answer(d['question'], d['context'])\n",
    "    print('Returned answer:', answer)\n",
    "    print('=========================================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f994d3-d73e-46ad-9c94-7255d982974c",
   "metadata": {},
   "source": [
    "# Task 11, 12, 13, 14, 15, 16\n",
    "#### Which pre-trained model performs better on that task?\n",
    "allegro_base (allegro/plt5-base), even though a majority of training questions were from not-polish SQUAD dataset!\n",
    "\n",
    "#### Does the performance on the validation dataset reflects the performance on your test set?\n",
    "No - only for allegro_small the validation and test metrics are similar. Potential reason explained below.\n",
    "\n",
    "#### What are the outcomes of the model on your own questions? Are they satisfying? If not, what might be the reason for that?\n",
    "Except for a few questions, the results are very satisfying. The answers often match the expected ones. Many differences stem from the inconsistencies in expected answers, for example\n",
    "- Question: Czy wyborca niepełnosprawny jest dopisywany do spisu wyborców?\n",
    "- Expected answer: \"Tak\"\n",
    "- Returned answer: \"na wniosek wniesiony do urzędu gminy najpóźniej w 10 dniu przed dniem wyborów\"\n",
    "\n",
    "In this example, both answers are correct, but f1 and *exact match* do not report it :(\n",
    "\n",
    "#### Why extractive question answering is not well suited for inflectional languages?\n",
    "Because the excerpt from the context might not be in the exact inflectional form we are looking for.\n",
    "\n",
    "#### Why you have to remove the duplicated questions from the training and the validation subsets?\n",
    "To exclude test questions from the training dataset. Otherwise, the results would be biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e31b36-0276-46df-bac8-d0584df16527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
