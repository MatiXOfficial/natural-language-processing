{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a332db-f05c-4aec-96e8-ba8db2b81da2",
   "metadata": {},
   "source": [
    "# Przetwarzanie języka naturalnego – lab4\n",
    "## Mateusz Kocot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e944e345-da3f-4df0-ae5c-94e283bde346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "from functools import reduce\n",
    "import math\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "import requests\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import regex as re\n",
    "from spacy.lang.pl import Polish as PolishSpacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20ea520e-4e86-46ff-8790-44d4df311290",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_JOBS = 8\n",
    "\n",
    "DATA_DIR = '../ustawy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0efff45-884f-456e-80fb-ad56874aa53c",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3fe0aae-24eb-4c46-ab04-b924f20539ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1179"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = {}\n",
    "\n",
    "for file_name in os.listdir(DATA_DIR):\n",
    "    with open(f'{DATA_DIR}/{file_name}', 'r', encoding='UTF-8') as file:\n",
    "        bill = file.read().lower()\n",
    "    dataset[file_name] = bill\n",
    "    \n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230f5ca5-ebc8-4ee2-9829-6015a86bd8f2",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56d23c24-62ce-44ec-b768-a2c99b1839e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_spacy = PolishSpacy()\n",
    "tokenizer = nlp_spacy.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "590a2ff7-e095-489f-a640-91286db29a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed 21.33 s\n"
     ]
    }
   ],
   "source": [
    "def tokenize(name, text):\n",
    "    return name, list(map(str, tokenizer(text)))\n",
    "\n",
    "def tokenize_collection(collection):\n",
    "    return sum(([tokenize(name, text)] for name, text in collection), [])\n",
    "    \n",
    "start = time.time()\n",
    "bill_items = list(dataset.items())\n",
    "tokenized_dataset = Parallel(n_jobs=N_JOBS)(delayed(tokenize_collection)(bill_items[i:i+100]) for i in range(0, len(bill_items), 100))\n",
    "tokenized_dataset = sum(tokenized_dataset, [])\n",
    "tokenized_dataset = dict(tokenized_dataset)\n",
    "print(f'Elapsed {time.time() - start:0.2f} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6935ce7-1369-4400-887b-276436e6539f",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "034d8e64-0a78-4ae3-bd8f-d589bc879767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('the', 'quick'): 2,\n",
       "         ('quick', 'brown'): 2,\n",
       "         ('brown', 'fox'): 2,\n",
       "         ('fox', 'jumps'): 1,\n",
       "         ('jumps', 'over'): 1,\n",
       "         ('over', 'the'): 1,\n",
       "         ('fox', '.'): 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_ngrams(text, n=2):\n",
    "    ngram_dict = defaultdict(int)\n",
    "    for i in range(len(text) - n + 1):\n",
    "        ngram = tuple(text[i:i+n]) if n > 1 else text[i]\n",
    "        ngram_dict[ngram] += 1\n",
    "    return Counter(ngram_dict)\n",
    "\n",
    "count_ngrams(['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'quick', 'brown', 'fox', '.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5a9ba9f-0361-4877-a10a-8c8be45f313e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed 14.79 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('art', '.'), 83778),\n",
       " (('ust', '.'), 53552),\n",
       " (('.', '\\n'), 49741),\n",
       " (('poz', '.'), 45198),\n",
       " ((',', 'poz'), 39655),\n",
       " (('-', '-'), 36542),\n",
       " (('r', '.'), 33010),\n",
       " (('w', 'art'), 30170),\n",
       " (('.', '1'), 29715),\n",
       " ((',', 'o'), 28739),\n",
       " (('mowa', 'w'), 27649),\n",
       " (('w', 'ust'), 22238),\n",
       " ((',', 'w'), 21526),\n",
       " (('2', '.'), 21274),\n",
       " (('1', '.'), 21111)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_ngrams_collection(collection, n=2):\n",
    "    return sum([count_ngrams(bill, n=n) for bill in collection], Counter())\n",
    "\n",
    "def compute_ngram_counts(dataset, n=2):\n",
    "    start = time.time()\n",
    "    bills = list(dataset.values())\n",
    "    ngram_counts = Parallel(n_jobs=N_JOBS)(delayed(count_ngrams_collection)(bills[i:i+100], n=n) for i in range(0, len(bills), 100))\n",
    "    ngram_counts = sum(ngram_counts, Counter())\n",
    "    print(f'Elapsed {time.time() - start:0.2f} s')\n",
    "    return ngram_counts\n",
    "\n",
    "bigram_counts = compute_ngram_counts(tokenized_dataset)\n",
    "\n",
    "bigram_counts.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c95474b-231f-445d-9064-288285baa3a9",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "436cac66-667f-49fd-a87b-42754b829ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('w', 'art'), 30170),\n",
       " (('mowa', 'w'), 27649),\n",
       " (('w', 'ust'), 22238),\n",
       " (('których', 'mowa'), 12973),\n",
       " (('o', 'których'), 12604),\n",
       " (('otrzymuje', 'brzmienie'), 9168),\n",
       " (('z', 'dnia'), 8989),\n",
       " (('którym', 'mowa'), 8689),\n",
       " (('o', 'którym'), 8525),\n",
       " (('do', 'spraw'), 8215)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fix_ngram_counts(ngram_counts, lemmatized=False):\n",
    "    fixed_bigram_counts = Counter()\n",
    "    for ngram, count in ngram_counts.items():\n",
    "        # if all tokens contain only letters\n",
    "        if all([not re.match(r'.*[^\\p{L}]', token if not lemmatized else token[0]) for token in ngram]):\n",
    "            fixed_bigram_counts[ngram] = count\n",
    "    return fixed_bigram_counts\n",
    "\n",
    "fixed_bigram_counts = fix_ngram_counts(bigram_counts)\n",
    "            \n",
    "fixed_bigram_counts.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e83570-6699-4a3f-b83d-93accb4865cd",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f585cdf-bba7-4900-b2f4-05c4141991f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('w', 201200),\n",
       " ('i', 90006),\n",
       " ('art', 83804),\n",
       " ('z', 82438),\n",
       " ('o', 64776),\n",
       " ('do', 60732),\n",
       " ('ust', 53636),\n",
       " ('na', 50643),\n",
       " ('się', 45886),\n",
       " ('lub', 45800)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_token_counts(dataset, lemmatized=False):\n",
    "    token_counts = sum([count_ngrams(bill, n=1) for name, bill in dataset.items()], Counter())\n",
    "    for token in list(token_counts.keys()):\n",
    "        # if token contains any non-letter character\n",
    "        if re.match(r'.*[^\\p{L}]', token if not lemmatized else token[0]): \n",
    "            del token_counts[token]\n",
    "    return token_counts\n",
    "\n",
    "token_counts = compute_token_counts(tokenized_dataset)\n",
    "\n",
    "token_counts.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac5bc35b-cc65-4335-a0ff-e738590180ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3566806"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size = sum(token_counts.values())\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81bc313d-8dda-4ee0-96e6-0193eb3a6853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.034908170336502"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_pmi(dataset_size, ngram_count, *token_counts):\n",
    "    frac = ngram_count / math.prod(token_counts)\n",
    "    for _ in range(len(token_counts) - 1):\n",
    "        frac *= dataset_size # multiply by the dataset size since we are dealing with probabilities\n",
    "    return np.log(frac)\n",
    "\n",
    "count_pmi(50_000_952, 1159, 1938, 1311)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e85aa00-0e75-4a1a-adfa-f08485645629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ngrams_pmi(fixed_ngram_counts, token_counts):\n",
    "    ngrams_pmi = {}\n",
    "    for ngram, ngram_count in fixed_ngram_counts.items():\n",
    "        single_token_counts = [token_counts[token] for token in ngram]\n",
    "        pmi = count_pmi(dataset_size, ngram_count, *single_token_counts)\n",
    "        ngrams_pmi[ngram] = pmi\n",
    "        \n",
    "    return ngrams_pmi\n",
    "\n",
    "bigrams_pmi = compute_ngrams_pmi(fixed_bigram_counts, token_counts)\n",
    "bigrams_pmi = Counter(bigrams_pmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346ad9ef-720d-451d-ab26-50fc32601f04",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7c50a35-4a37-4742-90dc-fcd3f7a72908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('kołowe', 'jednoosiowe'), 15.087181075421553),\n",
       " (('zbrojeń', 'żelbeto'), 15.087181075421553),\n",
       " (('prefabrykatów', 'wnętrzowe'), 15.087181075421553),\n",
       " (('gołe', 'aluminiowe'), 15.087181075421553),\n",
       " (('polistyrenu', 'spienionego'), 15.087181075421553),\n",
       " (('objaśnieniem', 'figur'), 15.087181075421553),\n",
       " (('wkładzie', 'wnoszonym'), 15.087181075421553),\n",
       " (('doktorem', 'habilitowanym'), 15.087181075421553),\n",
       " (('losy', 'loteryjne'), 15.087181075421553),\n",
       " (('ugaszone', 'zapałki'), 15.087181075421553)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_pmi.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674913b6-f004-4c9e-8bb8-f54dcd25de4f",
   "metadata": {},
   "source": [
    "# Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88e2fe39-a52e-46d7-9014-4099004cf9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('świeckie', 'przygotowujące'), 13.477743162987451),\n",
       " (('klęskami', 'żywiołowymi'), 13.477743162987451),\n",
       " (('ręcznego', 'miotacza'), 13.477743162987451),\n",
       " (('stajnią', 'wyścigową'), 13.477743162987451),\n",
       " (('otworami', 'wiertniczymi'), 13.477743162987451),\n",
       " (('obcowania', 'płciowego'), 13.477743162987451),\n",
       " (('młyny', 'kulowe'), 13.477743162987451),\n",
       " (('młynki', 'młotkowe'), 13.477743162987451),\n",
       " (('zaszkodzić', 'wynikom'), 13.477743162987451),\n",
       " (('grzegorz', 'schetyna'), 13.477743162987451)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_bigrams_pmi = Counter(dict(filter(lambda x: fixed_bigram_counts[x[0]] >= 5, bigrams_pmi.items())))\n",
    "\n",
    "filtered_bigrams_pmi.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f4e3ae-f6e1-4c3f-a275-91e4ce1c9188",
   "metadata": {},
   "source": [
    "# Task 7\n",
    "\n",
    "I used https://hub.docker.com/r/djstrong/krnnt/ instead of https://hub.docker.com/r/djstrong/krnnt2. The `output_format` option does not work in the latter one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "024d10fb-4c69-474b-945a-a6f658f72ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['ala', 'Ala', 'subst:sg:nom:f'], ['miała', 'mieć', 'praet:sg:f:imperf'], ['kota', 'kot', 'subst:sg:acc:m2'], [',', ',', 'interp'], ['ale', 'ale', 'conj'], ['już', 'już', 'qub'], ['nie', 'nie', 'qub'], ['ma', 'mieć', 'fin:sg:ter:imperf'], ['.', '.', 'interp']]]\n"
     ]
    }
   ],
   "source": [
    "print(requests.post('http://localhost:9003/?output_format=jsonl', data='ala miała kota, ale już nie ma.'.encode('utf-8')).json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54075da6-1251-480b-ad6c-035baf91d670",
   "metadata": {},
   "source": [
    "##### At first I lemmatize the bills and save the results using pickle. Elapsed time: 22 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51e23369-cfc0-4bcf-8339-5a17a75e68fb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    lemmatized_text = sum(requests.post('http://localhost:9003/?output_format=jsonl', data=text.encode('utf-8')).json(), [])\n",
    "    lemmatized_tokens = list(map(lambda x: (x[1], x[2].split(':', 1)[0]), lemmatized_text))\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# start = time.time()\n",
    "# for i, (name, bill) in enumerate(dataset.items()):\n",
    "#     if i % 10 == 0:\n",
    "#         print(f'Processing bill no {i}')\n",
    "#     lemmatized_text = lemmatize(bill)\n",
    "#     with open(f'lemmatized_corpus/{name[:-4]}.pkl', 'wb') as file:\n",
    "#         pickle.dump(lemmatized_text, file)\n",
    "# print(f'Elapsed {time.time() - start:0.2f} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a84991b-70fa-4da5-9f70-6442d517668c",
   "metadata": {},
   "source": [
    "##### Now I can load lemmatized bills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0d678e4-2a7c-43d1-a047-2c3fed8b5eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_dataset = {}\n",
    "for name in dataset.keys():\n",
    "    with open(f'lemmatized_corpus/{name[:-4]}.pkl', 'rb') as file:\n",
    "        lemmatized_dataset[name] = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2b5dd6f-7eb9-4d2c-86e8-d142bc65e5ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dziennik Ustaw', 'brev'),\n",
       " ('.', 'interp'),\n",
       " ('z', 'prep'),\n",
       " ('1993', 'adj'),\n",
       " ('rok', 'brev'),\n",
       " ('.', 'interp'),\n",
       " ('numer', 'brev'),\n",
       " ('129', 'num'),\n",
       " (',', 'interp'),\n",
       " ('pozycja', 'brev')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(lemmatized_dataset.values())[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a02879-1bc7-4ad0-82c3-1c43a4124c02",
   "metadata": {},
   "source": [
    "# Task 8 and 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b044f2e-1e48-4d0c-b337-0dc0303d4c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed 11.54 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((('artykuł', 'brev'), ('.', 'interp')), 83776),\n",
       " ((('ustęp', 'brev'), ('.', 'interp')), 53390),\n",
       " ((('pozycja', 'brev'), ('.', 'interp')), 45221),\n",
       " (((',', 'interp'), ('pozycja', 'brev')), 43184),\n",
       " ((('.', 'interp'), ('1', 'adj')), 39945),\n",
       " ((('-', 'interp'), ('-', 'interp')), 36580),\n",
       " ((('rok', 'brev'), ('.', 'interp')), 33026),\n",
       " ((('w', 'prep'), ('artykuł', 'brev')), 32039),\n",
       " (((',', 'interp'), ('o', 'prep')), 29908),\n",
       " ((('o', 'prep'), ('który', 'adj')), 28655),\n",
       " ((('który', 'adj'), ('mowa', 'subst')), 28538),\n",
       " ((('mowa', 'subst'), ('w', 'prep')), 28467),\n",
       " ((('.', 'interp'), ('2', 'adj')), 26032),\n",
       " ((('w', 'prep'), ('ustęp', 'brev')), 23554),\n",
       " ((('.', 'interp'), ('artykuł', 'brev')), 22921)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_bigram_counts = compute_ngram_counts(lemmatized_dataset)\n",
    "lem_bigram_counts.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fda80798-2f39-49e3-84ca-66690574639b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((('w', 'prep'), ('artykuł', 'brev')), 32039),\n",
       " ((('o', 'prep'), ('który', 'adj')), 28655),\n",
       " ((('który', 'adj'), ('mowa', 'subst')), 28538),\n",
       " ((('mowa', 'subst'), ('w', 'prep')), 28467),\n",
       " ((('w', 'prep'), ('ustęp', 'brev')), 23554),\n",
       " ((('z', 'prep'), ('dzień', 'subst')), 11360),\n",
       " ((('otrzymywać', 'fin'), ('brzmienie', 'subst')), 10529),\n",
       " ((('określić', 'ppas'), ('w', 'prep')), 10019),\n",
       " ((('do', 'prep'), ('sprawa', 'subst')), 8697),\n",
       " ((('ustawa', 'subst'), ('z', 'prep')), 8625)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove bigrams with tokens conatining non-alphanumeric characters\n",
    "fixed_lem_bigram_counts = fix_ngram_counts(lem_bigram_counts, lemmatized=True)\n",
    "fixed_lem_bigram_counts.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a694f66f-90cc-4f4c-9e47-3e47a2c6c419",
   "metadata": {},
   "source": [
    "# Task 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "902f58f4-0b80-4afd-ac8f-bae920023fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('w', 'prep'), 202659),\n",
       " (('i', 'conj'), 90025),\n",
       " (('z', 'prep'), 87989),\n",
       " (('artykuł', 'brev'), 83792),\n",
       " (('o', 'prep'), 64690),\n",
       " (('do', 'prep'), 60757),\n",
       " (('ustęp', 'brev'), 53449),\n",
       " (('na', 'prep'), 50647),\n",
       " (('który', 'adj'), 49383),\n",
       " (('się', 'qub'), 45888)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_token_counts = compute_token_counts(lemmatized_dataset, lemmatized=True)\n",
    "lem_token_counts.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4511d9d-3b90-4b5a-85e5-345d84b5d8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_bigrams_pmi = compute_ngrams_pmi(fixed_lem_bigram_counts, lem_token_counts)\n",
    "lem_bigrams_pmi = Counter(lem_bigrams_pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5f441b9-1010-474e-ba5f-101ce29095b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((('tornister', 'subst'), ('nieskórzany', 'adj')), 15.087181075421553),\n",
       " ((('cji', 'xxx'), ('wadociągowych', 'adj')), 15.087181075421553),\n",
       " ((('zbrojenia', 'subst'), ('żelbeto', 'adja')), 15.087181075421553),\n",
       " ((('reduktor', 'subst'), ('membranowy', 'adj')), 15.087181075421553),\n",
       " ((('prefabrykat', 'subst'), ('wnętrzowy', 'adj')), 15.087181075421553),\n",
       " ((('polistyren', 'subst'), ('spienić', 'ppas')), 15.087181075421553),\n",
       " ((('UW', 'subst'), ('zględnieniu', 'subst')), 15.087181075421553),\n",
       " ((('któ', 'adj'), ('rych', 'adj')), 15.087181075421553),\n",
       " ((('zaniedbać', 'ppas'), ('wychowawczo', 'adv')), 15.087181075421553),\n",
       " ((('english', 'subst'), ('language', 'xxx')), 15.087181075421553)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Before filtering:')\n",
    "lem_bigrams_pmi.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8233ab67-e59e-4357-9fd4-b8585ea3283c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((('młynek', 'subst'), ('młotkowy', 'adj')), 13.477743162987451),\n",
       " ((('Grzegorz', 'subst'), ('schetyna', 'subst')), 13.477743162987451),\n",
       " ((('łańcuchowy', 'adj'), ('rozszczepienie', 'subst')), 13.477743162987451),\n",
       " ((('pasta', 'subst'), ('emulsyjny', 'adj')), 13.295421606193496),\n",
       " ((('chrom', 'subst'), ('sześciowartościowy', 'adj')), 13.295421606193496),\n",
       " ((('Adam', 'subst'), ('Mickiewicz', 'subst')), 13.295421606193496),\n",
       " ((('młyn', 'subst'), ('kulowy', 'adj')), 13.14127092636624),\n",
       " ((('środa', 'subst'), ('wlkp', 'brev')), 13.14127092636624),\n",
       " ((('Piotrków', 'subst'), ('trybunalski', 'adj')), 13.14127092636624),\n",
       " ((('przeponowy', 'adj'), ('rurowy', 'adj')), 13.007739533741717)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_lem_bigrams_pmi = Counter(dict(filter(lambda x: fixed_lem_bigram_counts[x[0]] >= 5, lem_bigrams_pmi.items())))\n",
    "\n",
    "print('After filtering:')\n",
    "filtered_lem_bigrams_pmi.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba0f344-88ea-4330-93a8-7799be1207d1",
   "metadata": {},
   "source": [
    "# Task 11 and 12\n",
    "\n",
    "I could not come up with any reasonable method for counting the PMI values of trigrams which would use the bigram values. I mean, we could multiply the PMIs of bigram_1 (token_1, token_2) and bigram_2 (token_2, token_3), then divide it by the bigram_1 and bigram_2 counts, multiply by the number of token_2 occurrences, and finally multiply by the number of trigram occurrences. The only thing we would gain is not having to know the number of occurrences of token_1 and token_3, however."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22697a54-3928-42d7-a3a9-7ac71278a9cf",
   "metadata": {},
   "source": [
    "## Tokenized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5ea17ac-2038-43cf-99e0-44e999d6bb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed 25.98 s\n",
      "Filtered most common trigrams\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('których', 'mowa', 'w'), 12506),\n",
       " (('mowa', 'w', 'ust'), 12388),\n",
       " (('o', 'których', 'mowa'), 11714),\n",
       " (('mowa', 'w', 'art'), 10995),\n",
       " (('którym', 'mowa', 'w'), 8429),\n",
       " (('o', 'którym', 'mowa'), 8040),\n",
       " (('której', 'mowa', 'w'), 5020),\n",
       " (('o', 'której', 'mowa'), 4731),\n",
       " (('właściwy', 'do', 'spraw'), 4434),\n",
       " (('minister', 'właściwy', 'do'), 4104)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_counts = compute_ngram_counts(tokenized_dataset, n=3)\n",
    "fixed_trigram_counts = fix_ngram_counts(trigram_counts)\n",
    "\n",
    "print('Filtered most common trigrams:')\n",
    "fixed_trigram_counts.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "178aed08-b8d5-40ff-8f72-ff7dae6b6bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 PMI after filtering:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('profilem', 'zaufanym', 'epuap'), 24.964420728246495),\n",
       " (('finałowego', 'turnieju', 'mistrzostw'), 24.700691896817204),\n",
       " (('przedwczesnego', 'wyrębu', 'drzewostanu'), 24.58039454219411),\n",
       " (('potwierdzonym', 'profilem', 'zaufanym'), 24.49791972955196),\n",
       " (('piłce', 'nożnej', 'uefa'), 24.457334449436882),\n",
       " (('cienką', 'sierścią', 'zwierzęcą'), 24.215420795440682),\n",
       " (('szybkiemu', 'postępowi', 'technicznemu'), 24.170475083736566),\n",
       " (('turnieju', 'mistrzostw', 'europy'), 24.170063645755036),\n",
       " (('grożącą', 'jemu', 'samemu'), 24.003079177159286),\n",
       " (('wypalonym', 'paliwem', 'jądrowym'), 23.951785882771734)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams_pmi = compute_ngrams_pmi(fixed_trigram_counts, token_counts)\n",
    "trigrams_pmi = Counter(trigrams_pmi)\n",
    "\n",
    "filtered_trigrams_pmi = Counter(dict(filter(lambda x: fixed_trigram_counts[x[0]] >= 5, trigrams_pmi.items())))\n",
    "\n",
    "print('Top 10 PMI after filtering:')\n",
    "filtered_trigrams_pmi.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3dc811-4448-4d8f-94df-4a0913b04ca1",
   "metadata": {},
   "source": [
    "## Lemmatized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32cdd6e9-df73-4e00-99ae-93328361bd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed 23.13 s\n",
      "Filtered most common trigrams\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((('o', 'prep'), ('który', 'adj'), ('mowa', 'subst')), 28534),\n",
       " ((('który', 'adj'), ('mowa', 'subst'), ('w', 'prep')), 28436),\n",
       " ((('mowa', 'subst'), ('w', 'prep'), ('ustęp', 'brev')), 13471),\n",
       " ((('mowa', 'subst'), ('w', 'prep'), ('artykuł', 'brev')), 12308),\n",
       " ((('ustawa', 'subst'), ('z', 'prep'), ('dzień', 'subst')), 8589),\n",
       " ((('właściwy', 'adj'), ('do', 'prep'), ('sprawa', 'subst')), 7945),\n",
       " ((('minister', 'subst'), ('właściwy', 'adj'), ('do', 'prep')), 7888),\n",
       " ((('w', 'prep'), ('droga', 'subst'), ('rozporządzenie', 'subst')), 4737),\n",
       " ((('zastępować', 'fin'), ('się', 'qub'), ('wyraz', 'subst')), 3653),\n",
       " ((('w', 'prep'), ('ustawa', 'subst'), ('z', 'prep')), 3646)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_trigram_counts = compute_ngram_counts(lemmatized_dataset, n=3)\n",
    "fixed_lem_trigram_counts = fix_ngram_counts(lem_trigram_counts, lemmatized=True)\n",
    "\n",
    "print('Filtered most common trigrams:')\n",
    "fixed_lem_trigram_counts.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e75e653a-21db-4df0-b631-3f5c13def361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 PMI after filtering:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((('porcelanowy', 'adj'), ('młyn', 'subst'), ('kulowy', 'adj')),\n",
       "  25.925866908793743),\n",
       " ((('wymiennik', 'subst'), ('przeponowy', 'adj'), ('rurowy', 'adj')),\n",
       "  25.322331886923486),\n",
       " ((('reakcja', 'subst'), ('łańcuchowy', 'adj'), ('rozszczepienie', 'subst')),\n",
       "  24.927338078682617),\n",
       " ((('finałowy', 'adj'), ('turniej', 'subst'), ('mistrzostwa', 'subst')),\n",
       "  24.633098605684676),\n",
       " ((('piłka', 'subst'), ('nożny', 'adj'), ('UEFA', 'subst')),\n",
       "  24.10316263571627),\n",
       " ((('turniej', 'subst'), ('mistrzostwa', 'subst'), ('europ', 'subst')),\n",
       "  23.844641245320407),\n",
       " ((('przedwczesny', 'adj'), ('wyrąb', 'subst'), ('drzewostan', 'subst')),\n",
       "  23.692172651676135),\n",
       " ((('mecz', 'subst'), ('piłka', 'subst'), ('nożny', 'adj')),\n",
       "  23.592337011950278),\n",
       " ((('profil', 'subst'), ('zaufany', 'adj'), ('epuap', 'subst')),\n",
       "  23.531875349475847),\n",
       " ((('milion', 'brev'), ('dolar', 'subst'), ('USA', 'subst')),\n",
       "  23.419758051355142)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_trigrams_pmi = compute_ngrams_pmi(fixed_lem_trigram_counts, lem_token_counts)\n",
    "lem_trigrams_pmi = Counter(lem_trigrams_pmi)\n",
    "\n",
    "filtered_lem_trigrams_pmi = Counter(dict(filter(lambda x: fixed_lem_trigram_counts[x[0]] >= 5, lem_trigrams_pmi.items())))\n",
    "\n",
    "print('Top 10 PMI after filtering:')\n",
    "filtered_lem_trigrams_pmi.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13536eb1-9399-4122-aebd-c8da39183e21",
   "metadata": {},
   "source": [
    "# Task 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30bd13a9-5401-4fdb-bfac-c031d402ffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_to_lem(tok_tuple):\n",
    "    string = ' '.join(tok_tuple)\n",
    "    return tuple(lemmatize(string))\n",
    "\n",
    "def lem_to_tok(lem_tuple):\n",
    "    return tuple((l[0].lower() for l in lem_tuple))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54326599-8a85-498f-8194-dc5421f405b4",
   "metadata": {},
   "source": [
    "## Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df59fe26-9294-4eb3-9b77-257e9df9544d",
   "metadata": {},
   "source": [
    "### Top 10 tokenized vs lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fd9880c9-3faf-401b-8d78-26d85cd2a6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>PMI (tokenized)</th>\n",
       "      <th>lemmatized tokens</th>\n",
       "      <th>PMI (lemmatized)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>('świeckie', 'przygotowujące')</td>\n",
       "      <td>13.477743</td>\n",
       "      <td>(('świecki', 'adj'), ('przygotowywać', 'pact'))</td>\n",
       "      <td>12.219282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>('klęskami', 'żywiołowymi')</td>\n",
       "      <td>13.477743</td>\n",
       "      <td>(('klęska', 'subst'), ('żywiołowy', 'adj'))</td>\n",
       "      <td>10.306923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>('ręcznego', 'miotacza')</td>\n",
       "      <td>13.477743</td>\n",
       "      <td>(('ręczny', 'adj'), ('miotacz', 'subst'))</td>\n",
       "      <td>11.398302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>('stajnią', 'wyścigową')</td>\n",
       "      <td>13.477743</td>\n",
       "      <td>(('stajnia', 'subst'), ('wyścigowy', 'adj'))</td>\n",
       "      <td>10.920516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>('otworami', 'wiertniczymi')</td>\n",
       "      <td>13.477743</td>\n",
       "      <td>(('otwór', 'subst'), ('wiertniczy', 'adj'))</td>\n",
       "      <td>11.098197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>('obcowania', 'płciowego')</td>\n",
       "      <td>13.477743</td>\n",
       "      <td>(('obcowanie', 'subst'), ('płciowy', 'adj'))</td>\n",
       "      <td>12.448124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>('młyny', 'kulowe')</td>\n",
       "      <td>13.477743</td>\n",
       "      <td>(('młyn', 'subst'), ('kulowy', 'adj'))</td>\n",
       "      <td>13.141271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>('młynki', 'młotkowe')</td>\n",
       "      <td>13.477743</td>\n",
       "      <td>(('młynek', 'subst'), ('młotkowy', 'adj'))</td>\n",
       "      <td>13.477743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>('zaszkodzić', 'wynikom')</td>\n",
       "      <td>13.477743</td>\n",
       "      <td>(('zaszkodzić', 'inf'), ('wynik', 'subst'))</td>\n",
       "      <td>7.275613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>('grzegorz', 'schetyna')</td>\n",
       "      <td>13.477743</td>\n",
       "      <td>(('Grzegorz', 'subst'), ('schetyna', 'subst'))</td>\n",
       "      <td>13.477743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           tokens  PMI (tokenized)  \\\n",
       "0  ('świeckie', 'przygotowujące')        13.477743   \n",
       "1     ('klęskami', 'żywiołowymi')        13.477743   \n",
       "2        ('ręcznego', 'miotacza')        13.477743   \n",
       "3        ('stajnią', 'wyścigową')        13.477743   \n",
       "4    ('otworami', 'wiertniczymi')        13.477743   \n",
       "5      ('obcowania', 'płciowego')        13.477743   \n",
       "6             ('młyny', 'kulowe')        13.477743   \n",
       "7          ('młynki', 'młotkowe')        13.477743   \n",
       "8       ('zaszkodzić', 'wynikom')        13.477743   \n",
       "9        ('grzegorz', 'schetyna')        13.477743   \n",
       "\n",
       "                                 lemmatized tokens  PMI (lemmatized)  \n",
       "0  (('świecki', 'adj'), ('przygotowywać', 'pact'))         12.219282  \n",
       "1      (('klęska', 'subst'), ('żywiołowy', 'adj'))         10.306923  \n",
       "2        (('ręczny', 'adj'), ('miotacz', 'subst'))         11.398302  \n",
       "3     (('stajnia', 'subst'), ('wyścigowy', 'adj'))         10.920516  \n",
       "4      (('otwór', 'subst'), ('wiertniczy', 'adj'))         11.098197  \n",
       "5     (('obcowanie', 'subst'), ('płciowy', 'adj'))         12.448124  \n",
       "6           (('młyn', 'subst'), ('kulowy', 'adj'))         13.141271  \n",
       "7       (('młynek', 'subst'), ('młotkowy', 'adj'))         13.477743  \n",
       "8      (('zaszkodzić', 'inf'), ('wynik', 'subst'))          7.275613  \n",
       "9   (('Grzegorz', 'subst'), ('schetyna', 'subst'))         13.477743  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok, tok_pmi = tuple(zip(*filtered_bigrams_pmi.most_common()[:10]))\n",
    "lem = [tok_to_lem(t) for t in tok]\n",
    "lem_pmi = [lem_bigrams_pmi[l] for l in lem]\n",
    "\n",
    "data = {\n",
    "    'tokens': [str(t) for t in tok],\n",
    "    'PMI (tokenized)': tok_pmi,\n",
    "    'lemmatized tokens': [str(l) for l in lem],\n",
    "    'PMI (lemmatized)': lem_pmi\n",
    "}\n",
    "\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2238a942-81af-41fe-9e0a-371d2fa37468",
   "metadata": {},
   "source": [
    "### Top 10 lemmatized vs tokenized\n",
    "\n",
    "This way it doesn't make a lot of sense, because we lose the forms of the words, but... why not :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3e09c898-b025-42ac-abf0-d707daf74ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemmatized tokens</th>\n",
       "      <th>PMI (lemmatized)</th>\n",
       "      <th>tokens</th>\n",
       "      <th>PMI (tokenized)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(('młynek', 'subst'), ('młotkowy', 'adj'))</td>\n",
       "      <td>13.477743</td>\n",
       "      <td>('młynek', 'młotkowy')</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(('Grzegorz', 'subst'), ('schetyna', 'subst'))</td>\n",
       "      <td>13.477743</td>\n",
       "      <td>('grzegorz', 'schetyna')</td>\n",
       "      <td>13.477743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(('łańcuchowy', 'adj'), ('rozszczepienie', 'su...</td>\n",
       "      <td>13.477743</td>\n",
       "      <td>('łańcuchowy', 'rozszczepienie')</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(('pasta', 'subst'), ('emulsyjny', 'adj'))</td>\n",
       "      <td>13.295422</td>\n",
       "      <td>('pasta', 'emulsyjny')</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(('chrom', 'subst'), ('sześciowartościowy', 'a...</td>\n",
       "      <td>13.295422</td>\n",
       "      <td>('chrom', 'sześciowartościowy')</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(('Adam', 'subst'), ('Mickiewicz', 'subst'))</td>\n",
       "      <td>13.295422</td>\n",
       "      <td>('adam', 'mickiewicz')</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(('młyn', 'subst'), ('kulowy', 'adj'))</td>\n",
       "      <td>13.141271</td>\n",
       "      <td>('młyn', 'kulowy')</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(('środa', 'subst'), ('wlkp', 'brev'))</td>\n",
       "      <td>13.141271</td>\n",
       "      <td>('środa', 'wlkp')</td>\n",
       "      <td>13.477743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(('Piotrków', 'subst'), ('trybunalski', 'adj'))</td>\n",
       "      <td>13.141271</td>\n",
       "      <td>('piotrków', 'trybunalski')</td>\n",
       "      <td>13.141271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(('przeponowy', 'adj'), ('rurowy', 'adj'))</td>\n",
       "      <td>13.007740</td>\n",
       "      <td>('przeponowy', 'rurowy')</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   lemmatized tokens  PMI (lemmatized)  \\\n",
       "0         (('młynek', 'subst'), ('młotkowy', 'adj'))         13.477743   \n",
       "1     (('Grzegorz', 'subst'), ('schetyna', 'subst'))         13.477743   \n",
       "2  (('łańcuchowy', 'adj'), ('rozszczepienie', 'su...         13.477743   \n",
       "3         (('pasta', 'subst'), ('emulsyjny', 'adj'))         13.295422   \n",
       "4  (('chrom', 'subst'), ('sześciowartościowy', 'a...         13.295422   \n",
       "5       (('Adam', 'subst'), ('Mickiewicz', 'subst'))         13.295422   \n",
       "6             (('młyn', 'subst'), ('kulowy', 'adj'))         13.141271   \n",
       "7             (('środa', 'subst'), ('wlkp', 'brev'))         13.141271   \n",
       "8    (('Piotrków', 'subst'), ('trybunalski', 'adj'))         13.141271   \n",
       "9         (('przeponowy', 'adj'), ('rurowy', 'adj'))         13.007740   \n",
       "\n",
       "                             tokens  PMI (tokenized)  \n",
       "0            ('młynek', 'młotkowy')         0.000000  \n",
       "1          ('grzegorz', 'schetyna')        13.477743  \n",
       "2  ('łańcuchowy', 'rozszczepienie')         0.000000  \n",
       "3            ('pasta', 'emulsyjny')         0.000000  \n",
       "4   ('chrom', 'sześciowartościowy')         0.000000  \n",
       "5            ('adam', 'mickiewicz')         0.000000  \n",
       "6                ('młyn', 'kulowy')         0.000000  \n",
       "7                 ('środa', 'wlkp')        13.477743  \n",
       "8       ('piotrków', 'trybunalski')        13.141271  \n",
       "9          ('przeponowy', 'rurowy')         0.000000  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem, lem_pmi = tuple(zip(*filtered_lem_bigrams_pmi.most_common()[:10]))\n",
    "tok = [lem_to_tok(l) for l in lem]\n",
    "tok_pmi = [bigrams_pmi[t] for t in tok]\n",
    "\n",
    "data = {\n",
    "    'lemmatized tokens': [str(l) for l in lem],\n",
    "    'PMI (lemmatized)': lem_pmi,\n",
    "    'tokens': [str(t) for t in tok],\n",
    "    'PMI (tokenized)': tok_pmi\n",
    "}\n",
    "\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa57de57-842e-40c2-b961-79698191322e",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f417bb60-651b-44d3-afa9-236b85774673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "papier wartościowy\n",
      "papier oszczędnościowy\n",
      "papier gazetowy\n",
      "papier toaletowy\n",
      "papier do\n"
     ]
    }
   ],
   "source": [
    "for a, b in bigrams_pmi:\n",
    "    if (a.endswith('papier')):\n",
    "        print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2105496-d406-4231-9a6f-1b15e6cca43c",
   "metadata": {},
   "source": [
    "#### Lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0996aa89-1cf9-4974-a275-399a2d60eef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.903017378323081"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_bigrams_pmi[tok_to_lem(('papier', 'wartościowy'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d6b1b812-4b45-460e-8efd-f378b76ff8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.852521019359115"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_bigrams_pmi[tok_to_lem(('papier', 'oszczędnościowy'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d31b145c-e525-4f00-b573-7cf1300092df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.943563472717431"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_bigrams_pmi[tok_to_lem(('papier', 'gazetowy'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e2bd6c1a-06bd-485f-a963-be9124153d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.238815380479005"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_bigrams_pmi[tok_to_lem(('papier', 'toaletowy'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f8f9f1-b627-45a0-83b0-4dc8f60a2961",
   "metadata": {},
   "source": [
    "#### Tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cdf6c64f-ed54-4cc7-a598-43c3523afd31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.609844260943346"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_pmi[('papier', 'wartościowy')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ed80a9c7-5d6e-467e-9294-870341e0603f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.492061225286962"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_pmi[('papier', 'oszczędnościowy')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b9743ab3-dd97-4098-8437-d3c9a66ad65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.590673513955071"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_pmi[('papier', 'gazetowy')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9a8cfc76-b27a-43db-97d6-ad3d48517d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.590673513955071"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_pmi[('papier', 'toaletowy')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f832a4-6bbe-4cb1-b5ba-f874ec9329e9",
   "metadata": {},
   "source": [
    "# Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdace387-52b0-4ab0-bfb5-742da3c0de6c",
   "metadata": {},
   "source": [
    "### Top 10 tokenized vs lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce68a194-bc01-4f48-b2c4-45f94c6c8170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>PMI (tokenized)</th>\n",
       "      <th>lemmatized tokens</th>\n",
       "      <th>PMI (lemmatized)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>('profilem', 'zaufanym', 'epuap')</td>\n",
       "      <td>24.964421</td>\n",
       "      <td>(('profil', 'subst'), ('zaufany', 'adj'), ('ep...</td>\n",
       "      <td>23.531875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>('finałowego', 'turnieju', 'mistrzostw')</td>\n",
       "      <td>24.700692</td>\n",
       "      <td>(('finałowy', 'adj'), ('turniej', 'subst'), ('...</td>\n",
       "      <td>24.633099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>('przedwczesnego', 'wyrębu', 'drzewostanu')</td>\n",
       "      <td>24.580395</td>\n",
       "      <td>(('przedwczesny', 'adj'), ('wyręb', 'subst'), ...</td>\n",
       "      <td>23.921014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>('potwierdzonym', 'profilem', 'zaufanym')</td>\n",
       "      <td>24.497920</td>\n",
       "      <td>(('potwierdzić', 'ppas'), ('profil', 'subst'),...</td>\n",
       "      <td>20.937769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>('piłce', 'nożnej', 'uefa')</td>\n",
       "      <td>24.457334</td>\n",
       "      <td>(('piłka', 'subst'), ('nożny', 'adj'), ('UEFA'...</td>\n",
       "      <td>24.103163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>('cienką', 'sierścią', 'zwierzęcą')</td>\n",
       "      <td>24.215421</td>\n",
       "      <td>(('cienki', 'adj'), ('sierść', 'subst'), ('zwi...</td>\n",
       "      <td>19.407921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>('szybkiemu', 'postępowi', 'technicznemu')</td>\n",
       "      <td>24.170475</td>\n",
       "      <td>(('szybki', 'adj'), ('postęp', 'subst'), ('tec...</td>\n",
       "      <td>16.570299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>('turnieju', 'mistrzostw', 'europy')</td>\n",
       "      <td>24.170064</td>\n",
       "      <td>(('turniej', 'subst'), ('mistrzostwa', 'subst'...</td>\n",
       "      <td>23.844641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>('grożącą', 'jemu', 'samemu')</td>\n",
       "      <td>24.003079</td>\n",
       "      <td>(('grozić', 'pact'), ('on', 'ppron3'), ('sam',...</td>\n",
       "      <td>10.169878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>('wypalonym', 'paliwem', 'jądrowym')</td>\n",
       "      <td>23.951786</td>\n",
       "      <td>(('wypalić', 'ppas'), ('paliwo', 'subst'), ('j...</td>\n",
       "      <td>17.617158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        tokens  PMI (tokenized)  \\\n",
       "0            ('profilem', 'zaufanym', 'epuap')        24.964421   \n",
       "1     ('finałowego', 'turnieju', 'mistrzostw')        24.700692   \n",
       "2  ('przedwczesnego', 'wyrębu', 'drzewostanu')        24.580395   \n",
       "3    ('potwierdzonym', 'profilem', 'zaufanym')        24.497920   \n",
       "4                  ('piłce', 'nożnej', 'uefa')        24.457334   \n",
       "5          ('cienką', 'sierścią', 'zwierzęcą')        24.215421   \n",
       "6   ('szybkiemu', 'postępowi', 'technicznemu')        24.170475   \n",
       "7         ('turnieju', 'mistrzostw', 'europy')        24.170064   \n",
       "8                ('grożącą', 'jemu', 'samemu')        24.003079   \n",
       "9         ('wypalonym', 'paliwem', 'jądrowym')        23.951786   \n",
       "\n",
       "                                   lemmatized tokens  PMI (lemmatized)  \n",
       "0  (('profil', 'subst'), ('zaufany', 'adj'), ('ep...         23.531875  \n",
       "1  (('finałowy', 'adj'), ('turniej', 'subst'), ('...         24.633099  \n",
       "2  (('przedwczesny', 'adj'), ('wyręb', 'subst'), ...         23.921014  \n",
       "3  (('potwierdzić', 'ppas'), ('profil', 'subst'),...         20.937769  \n",
       "4  (('piłka', 'subst'), ('nożny', 'adj'), ('UEFA'...         24.103163  \n",
       "5  (('cienki', 'adj'), ('sierść', 'subst'), ('zwi...         19.407921  \n",
       "6  (('szybki', 'adj'), ('postęp', 'subst'), ('tec...         16.570299  \n",
       "7  (('turniej', 'subst'), ('mistrzostwa', 'subst'...         23.844641  \n",
       "8  (('grozić', 'pact'), ('on', 'ppron3'), ('sam',...         10.169878  \n",
       "9  (('wypalić', 'ppas'), ('paliwo', 'subst'), ('j...         17.617158  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok, tok_pmi = tuple(zip(*filtered_trigrams_pmi.most_common()[:10]))\n",
    "lem = [tok_to_lem(t) for t in tok]\n",
    "lem_pmi = [lem_trigrams_pmi[l] for l in lem]\n",
    "\n",
    "data = {\n",
    "    'tokens': [str(t) for t in tok],\n",
    "    'PMI (tokenized)': tok_pmi,\n",
    "    'lemmatized tokens': [str(l) for l in lem],\n",
    "    'PMI (lemmatized)': lem_pmi\n",
    "}\n",
    "\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb4c3b5-5317-4bc1-bcd6-3f05b4a2bdb5",
   "metadata": {},
   "source": [
    "### Top 10 lemmatized vs tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73ff6e0d-849f-4239-8af1-0d3ee4a5ff6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemmatized tokens</th>\n",
       "      <th>PMI (lemmatized)</th>\n",
       "      <th>tokens</th>\n",
       "      <th>PMI (tokenized)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(('porcelanowy', 'adj'), ('młyn', 'subst'), ('...</td>\n",
       "      <td>25.925867</td>\n",
       "      <td>('porcelanowy', 'młyn', 'kulowy')</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(('wymiennik', 'subst'), ('przeponowy', 'adj')...</td>\n",
       "      <td>25.322332</td>\n",
       "      <td>('wymiennik', 'przeponowy', 'rurowy')</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(('reakcja', 'subst'), ('łańcuchowy', 'adj'), ...</td>\n",
       "      <td>24.927338</td>\n",
       "      <td>('reakcja', 'łańcuchowy', 'rozszczepienie')</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(('finałowy', 'adj'), ('turniej', 'subst'), ('...</td>\n",
       "      <td>24.633099</td>\n",
       "      <td>('finałowy', 'turniej', 'mistrzostwa')</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(('piłka', 'subst'), ('nożny', 'adj'), ('UEFA'...</td>\n",
       "      <td>24.103163</td>\n",
       "      <td>('piłka', 'nożny', 'uefa')</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(('turniej', 'subst'), ('mistrzostwa', 'subst'...</td>\n",
       "      <td>23.844641</td>\n",
       "      <td>('turniej', 'mistrzostwa', 'europ')</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(('przedwczesny', 'adj'), ('wyrąb', 'subst'), ...</td>\n",
       "      <td>23.692173</td>\n",
       "      <td>('przedwczesny', 'wyrąb', 'drzewostan')</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(('mecz', 'subst'), ('piłka', 'subst'), ('nożn...</td>\n",
       "      <td>23.592337</td>\n",
       "      <td>('mecz', 'piłka', 'nożny')</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(('profil', 'subst'), ('zaufany', 'adj'), ('ep...</td>\n",
       "      <td>23.531875</td>\n",
       "      <td>('profil', 'zaufany', 'epuap')</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(('milion', 'brev'), ('dolar', 'subst'), ('USA...</td>\n",
       "      <td>23.419758</td>\n",
       "      <td>('milion', 'dolar', 'usa')</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   lemmatized tokens  PMI (lemmatized)  \\\n",
       "0  (('porcelanowy', 'adj'), ('młyn', 'subst'), ('...         25.925867   \n",
       "1  (('wymiennik', 'subst'), ('przeponowy', 'adj')...         25.322332   \n",
       "2  (('reakcja', 'subst'), ('łańcuchowy', 'adj'), ...         24.927338   \n",
       "3  (('finałowy', 'adj'), ('turniej', 'subst'), ('...         24.633099   \n",
       "4  (('piłka', 'subst'), ('nożny', 'adj'), ('UEFA'...         24.103163   \n",
       "5  (('turniej', 'subst'), ('mistrzostwa', 'subst'...         23.844641   \n",
       "6  (('przedwczesny', 'adj'), ('wyrąb', 'subst'), ...         23.692173   \n",
       "7  (('mecz', 'subst'), ('piłka', 'subst'), ('nożn...         23.592337   \n",
       "8  (('profil', 'subst'), ('zaufany', 'adj'), ('ep...         23.531875   \n",
       "9  (('milion', 'brev'), ('dolar', 'subst'), ('USA...         23.419758   \n",
       "\n",
       "                                        tokens  PMI (tokenized)  \n",
       "0            ('porcelanowy', 'młyn', 'kulowy')                0  \n",
       "1        ('wymiennik', 'przeponowy', 'rurowy')                0  \n",
       "2  ('reakcja', 'łańcuchowy', 'rozszczepienie')                0  \n",
       "3       ('finałowy', 'turniej', 'mistrzostwa')                0  \n",
       "4                   ('piłka', 'nożny', 'uefa')                0  \n",
       "5          ('turniej', 'mistrzostwa', 'europ')                0  \n",
       "6      ('przedwczesny', 'wyrąb', 'drzewostan')                0  \n",
       "7                   ('mecz', 'piłka', 'nożny')                0  \n",
       "8               ('profil', 'zaufany', 'epuap')                0  \n",
       "9                   ('milion', 'dolar', 'usa')                0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem, lem_pmi = tuple(zip(*filtered_lem_trigrams_pmi.most_common()[:10]))\n",
    "tok = [lem_to_tok(l) for l in lem]\n",
    "tok_pmi = [trigrams_pmi[t] for t in tok]\n",
    "\n",
    "data = {\n",
    "    'lemmatized tokens': [str(l) for l in lem],\n",
    "    'PMI (lemmatized)': lem_pmi,\n",
    "    'tokens': [str(t) for t in tok],\n",
    "    'PMI (tokenized)': tok_pmi\n",
    "}\n",
    "\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1c6247-4148-4e71-a2c2-67d77fd6136c",
   "metadata": {},
   "source": [
    "# Task 14\n",
    "\n",
    "#### Why do we have to filter the bigrams, rather than the token sequence?\n",
    "This way we avoid creating bigrams composed of tokens from separate sentences, paragraphs, etc. Let's consider this example: \"Ula ma psa. Pies ma Ulę\". If we filter the token sequence before creating bigrams, we will end up with \"psa pies\".\n",
    "\n",
    "#### Which method works better for the bigrams and which for the trigrams?\n",
    "- Tokenization seems to be working a bit better for bigrams. Most often it performs similarly to lemmatization, but there are words which can make a lot of collocations, and they are more likely to be discovered using tokenization. For example, let's take the word \"papier\". It is used together with \"wartościowy\", \"oszczędnościowy\", \"gazetowy\" and \"toaletowy\". As found in the example a few cells above, these collocations are easier to discover using the tokenized corpus.\n",
    "- On the other hand, lemmatization appears to be better for trigrams. The tokenization approach assigned high PMIs to a few phrases that aren't collocations, e.g. 'cienką sierścią zwierzęcą', 'grożącą jemu samemu'. This effect occurred much less often for bigrams. The lemmatized trigrams with high PMI are more reasonable.\n",
    "\n",
    "#### What types of expressions are discovered by the methods?\n",
    "- names, e.g. \"Adam Mickiewicz\"\n",
    "- proper nouns, e.g. \"Środa Wielkopolska\"\n",
    "- proper nouns with common descriptions, e.g \"profil zaufany EPUAP\", \"mecz piłki nożnej\"\n",
    "- words that together form another meaning (noun + adj, noun + noun), e.g. \"ręczny miotacz\", \"turniej mistrzostwa europy\", \"otwór wiertniczy\"\n",
    "- words that are just often used together (common adjective, etc.), e.g. \"przedwczesny wyrąb drzewostanu\", \"potwierdzony profil zaufany\", \"szybki postęp techniczny\"\n",
    "- random phrases, e.g. \"grożącą jemu samemu\"\n",
    "\n",
    "#### Can you devise a different type of filtering that would yield better results?\n",
    "Apart from removing words containing non-letter characters, we could also remove some common words (mostly conjunctions), like 'w', 'do', 'a', 'ja', 'ty', 'i', 'dlatego', etc. It would remove phrases like \"papier do\" from the dataset, which would strengthen collocations like \"papier wartościowy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b22557f-bef5-4a43-ae99-57144c5bb504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
